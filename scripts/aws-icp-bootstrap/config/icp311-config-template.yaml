
---

## Network Settings
network_type: calico
# network_helm_chart_path: < helm chart path >

## Network in IPv4 CIDR format
network_cidr: ${CLUSTER_CIDR}

## Kubernetes Settings
service_cluster_ip_range: ${SERVICE_CIDR}

cluster_domain: ${CLUSTER_DOMAIN}
cluster_name: ${CLUSTER_NAME}
cluster_CA_domain: ${CLUSTER_CA_DOMAIN}

## Etcd Settings
etcd_extra_args: ["--grpc-keepalive-timeout=0", "--grpc-keepalive-interval=0", "--snapshot-count=10000"]
# Keep the log data separate from the etcd data.
# You could set etcd wal dirctory to a centralized and remote log directory for persistent logging.
# etcd_data_dir: "/var/lib/etcd"
# etcd_wal_dir: "/var/lib/etcd-wal"

## General Settings
# wait_for_timeout: 600

## Advanced Settings
default_admin_user: ${ADMIN_USER}
default_admin_password: ${ADMIN_PASSWORD}
# ansible_user: <username>
# ansible_become: true
# ansible_become_password: <password>

## Kubernetes Settings
# kubelet_extra_args: [""]
# kube_apiserver_extra_args: []
# kube_controller_manager_extra_args: []
# kube_proxy_extra_args: []
# kube_scheduler_extra_args: []

## Bootstrap token
# bootstrap_token_ttl: "24h0m0s"


## Enable Kubernetes Audit Log
# auditlog_enabled: false

## Cluster Router settings
# router_http_port: 8080
# router_https_port: 8443

## Nginx Ingress settings
# ingress_http_port: 80
# ingress_https_port: 443

## GlusterFS Storage Settings
# storage-glusterfs:
#  nodes:
#    - ip: <storage_node_m_IP_address>
#      devices:
#        - <link path>/<symlink of device aaa>
#        - <link path>/<symlink of device bbb>
#    - ip: <storage_node_n_IP_address>
#      devices:
#        - <link path>/<symlink of device ccc>
#    - ip: <storage_node_o_IP_address>
#      devices:
#        - <link path>/<symlink of device ddd>
#  storageClass:
#    create: true
#    name: glusterfs
#    default: false
#    volumetype: replicate:3
#    reclaimPolicy: Delete
#    volumeBindingMode: Immediate
#    volumeNamePrefix: icp
#    additionalProvisionerParams: {}
#    allowVolumeExpansion: true
#  gluster:
#    resources:
#      requests:
#        cpu: 100m
#        memory: 128Mi
#      limits:
#        cpu: 200m
#        memory: 256Mi
#  heketi:
#    backupDbSecret: heketi-db-backup
#    authSecret: heketi-secret
#    resources:
#      requests:
#        cpu: 500m
#        memory: 512Mi
#      limits:
#        cpu: 1000m
#        memory: 1Gi
#  nodeSelector:
#    key: hostgroup
#    value: glusterfs
#  prometheus:
#    enabled: false
#    path: "/metrics"
#    port: 8080
#  tolerations: []


## storage-minio settings
# storage-minio:
#  image:
#    repository: "{{ image_repo }}/minio"
#  mcImage:
#    repository: "{{ image_repo }}/minio-mc"
#  mode: standalone
#  accessKey: "admin"
#  secretKey: "admin1234"
#  minioAccessSercret: "minio-secret"
#  configPath: "/root/.minio/"
#  mountPath: "/export"
#  replica: 4
#  persistence:
#    enabled: false
#    useDynamicProvisioning: false
#    storageClass: standard
#    accessMode: ReadWriteOnce
#    size: 10Gi
#  service:
#    type: ClusterIP
#    clusterIP: None
#    loadBalancerIP: None
#    port: 9000
#    nodePort: 31311
#  ingress:
#    enabled: false
#    path: /
#    hosts: ""
#    tls: ""
#  tls:
#    enabled: false
#    type: "selfsigned"
#    minioTlsSercret: ""
#  nodeSelector: ""
#  tolerations: ""

## Network Settings
## Calico Network Settings
# calico_ipip_enabled: true
calico_tunnel_mtu: ${CALICO_TUNNEL_MTU}
# calico_ip_autodetection_method: can-reach={{ groups['master'][0] }}

## IPSec mesh Settings
## If user wants to configure IPSec mesh, the following parameters
## should be configured through config.yaml
# ipsec_mesh:
#   enable: true
#   interface: <interface name on which IPsec will be enabled>
#   subnets: []
#   exclude_ips: []

## Environment Isolation
# Example:[{production:prod}, {devops:dev}, {preproduction:preprod}]
isolated_namespaces: []
isolated_proxies: []

# kube_apiserver_secure_port: 8001

## External loadbalancer IP or domain
## Or floating IP in OpenStack environment
cluster_lb_address: ${CLUSTER_LB_ADDRESS}

## External loadbalancer IP or domain
## Or floating IP in OpenStack environment
proxy_lb_address: ${PROXY_LB_ADDRESS}

## Install in firewall enabled mode
# firewall_enabled: false

## Allow loopback dns server in cluster nodes
# loopback_dns: false

## High Availability Settings: etcd or keepalived
vip_manager: etcd

## High Availability Settings for master nodes
#vip_iface: ${CLUSTER_VIP_IFACE}
#cluster_vip: ${CLUSTER_VIP}

## High Availability Settings for Proxy nodes
#proxy_vip_iface: ${PROXY_VIP_IFACE}
#proxy_vip: ${PROXY_VIP}

## vSphere cloud provider Settings
## If user wants to configure vSphere as cloud provider, vsphere_conf
## parameters should be configured through config.yaml
kubelet_nodename: ${KUBLET_NODENAME}
cloud_provider: ${CLOUD_PROVIDER}
# vsphere_conf:
#    user: <vCenter username for vSphere cloud provider>
#    password: <password for vCenter user>
#    server: <vCenter server IP or FQDN>
#    port: [vCenter Server Port; default: 443]
#    insecure_flag: [set to 1 if vCenter uses a self-signed certificate]
#    datacenter: <datacenter name on which Node VMs are deployed>
#    datastore: <default datastore to be used for provisioning volumes>
#    working_dir: <vCenter VM folder path in which node VMs are located>

## You can disable following services if they are not needed:
#   custom-metrics-adapter
#   image-security-enforcement
#   istio
#   metering
#   monitoring
#   service-catalog
#   storage-minio
#   storage-glusterfs
#   vulnerability-advisor
management_services:
  custom-metrics-adapter: ${CUSTOM_METRICS_ADAPTER}
  image-security-enforcement: ${IMAGE_SECURITY_ENFORCEMENT}
  istio: ${ISTIO}
  metering: ${METERING}
  monitoring: ${MONITORING}
  service-catalog: ${SERVICE_CATALOG}
  storage-minio: ${MINIO}
  storage-glusterfs: ${GLUSTERFS}
  vulnerability-advisor: ${VULNERABILITY_ADVISOR}

# ICP 3.1 KC indicates to add this to the config.yaml
# PVS: Each policy has a name and a policy attribute, but
# policy is always empty (as far as I've seen).
# PVS: The long list of image policies is based on what is 
# needed for the collection of helm charts in the standard helm
# repos that are available out-of-the-box.
# All values need to be quoted to avoid YAML syntax issues, e.g., a colon in the value.
image-security-enforcement:
  clusterImagePolicy:
    - name: "${CLUSTER_DNS_NAME}:8500/*"
    - name: "docker.io/ibmcom/*"
    - name: "docker.io/hybridcloudibm/*"
    - name: "docker.io/db2eventstore/*"
    - name: "docker.io/icpdashdb/*"
    - name: "docker.io/iighostd/*"
    - name: "docker.io/store/ibmcorp/*"
    - name: "docker.io/alpine*"
    - name: "docker.io/busybox*"
    - name: "docker.io/dduportal/bats:*"
    - name: "docker.io/cassandra:*"
    - name: "docker.io/haproxy:*"
    - name: "docker.io/hazelcast/hazelcast-kubernetes:*"
    - name: "docker.io/library/busybox:*"
    - name: "docker.io/minio/mc:*"
    - name: "docker.io/minio/minio:*"
    - name: "docker.io/nginx:*"
    - name: "docker.io/open-liberty:*"
    - name: "docker.io/rabbitmq:*"
    - name: "docker.io/radial/busyboxplus:*"
    - name: "docker.io/ubuntu*"
    - name: "docker.io/websphere-liberty:*"
    - name: "docker.io/wurstmeister/kafka:*"
    - name: "docker.io/zookeeper:*"
    - name: "docker.io/ibmcloudcontainers/strongswan:*"
    - name: "docker.io/opsh2oai/dai-ppc64le:*"
    - name: "docker.io/redis*"
    - name: "docker.io/f5networks/k8s-bigip-ctlr:*"
    - name: "docker.io/rook/rook:*"
    - name: "docker.io/couchdb:*"
    - name: "docker.elastic.co/beats/filebeat:*"
    - name: "docker.io/prom/statsd-exporter:*"
    - name: "docker.elastic.co/elasticsearch/elasticsearch:*"
    - name: "docker.elastic.co/kibana/kibana:*"
    - name: "docker.elastic.co/logstash/logstash:*"
    - name: "quay.io/k8scsi/csi-attacher:*"
    - name: "quay.io/k8scsi/driver-registrar:*"
    - name: "quay.io/k8scsi/nfsplugin:*"
    - name: "quay.io/external_storage/efs-provisioner:*"
    - name: "k8s.gcr.io/hyperkube:*"
    - name: "registry.bluemix.net/ibm/*"
    - name: "registry.bluemix.net/armada-master/ibm-worker-recovery:*"
      
## Docker configuration option, more options see
## https://docs.docker.com/engine/reference/commandline/dockerd/#daemon-configuration-file
# docker_config:
#   log-opts:
#     max-size: "100m"
#     max-file: "10"

## Docker environment setup
# docker_env:
#   - HTTP_PROXY=http://1.2.3.4:3128
#   - HTTPS_PROXY=http://1.2.3.4:3128
#   - NO_PROXY=localhost,127.0.0.1,{{ cluster_CA_domain }}

## Install/upgrade docker version
# docker_version: 18.03.1

## Install Docker automatically or not
install_docker: false

## Nginx Ingress Controller configuration
## You can add your nginx ingress controller configuration, and the allowed configuration can refer to
## https://github.com/kubernetes/ingress-nginx/blob/nginx-0.16.2/docs/user-guide/nginx-configuration/configmap.md
## Section ingress_controller is obsolete, it is replaced by nginx-ingress.
# nginx-ingress:
#   ingress:
#     config:
#       disable-access-log: 'true'
#       keep-alive-requests: '10000'
#       upstream-keepalive-connections: '64'
#       worker-processes: "2"
#     extraArgs:
#       publish-status-address: "{{ proxy_external_address }}"
#       enable-ssl-passthrough: true

## Clean metrics indices in Elasticsearch older than this number of days
# metrics_max_age: 1

## Clean application log indices in Elasticsearch older than this number of days
# logs_maxage: 1

## Istio addons security Settings
## If user wants to configure Istio addons securty settings
## parameters should be configured through config.yaml
# istio_addon:
#   grafana:
#     username: admin
#     passphrase: admin
#   kiali:
#     username: admin
#     passphrase: admin